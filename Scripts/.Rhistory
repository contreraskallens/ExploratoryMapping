row.names(replicationProjection) <- row.names(repFreqMatrix) #set row names as wholeMacaroni
colnames(replicationProjection) <- colnames(freqMatrix)
sharedWords <- colnames(repFreqMatrix)[which(colnames(repFreqMatrix) %in% colnames(freqMatrix))] # obtain all the shared words between original data and replication data.
replicationProjection[,sharedWords] <- repFreqMatrix[,sharedWords] #build a new DbT that has the documents in replication as rows and the shared words between both datasets as columns.
cross_svd <- replicationProjection %*% ginv(termLoadings) #projects the replication data into the SV  space of the original dataset..
}
### TOPIC BEST PREDICTORS ###
## At first, the dimensions that differentiate the papers of a theory when compared to the other ones are selected. The first 80 of them are stored. #
##FREE FOR ALL##
#for each topic, fill the "best predictors" matrix with the dimensions that most differentiate that theory from the other 8 theories.
bestPredictors <- c()
predictorRatings <- c() #store ratings to compare positive versus negative in term loading inspection
for (topic in topicList) {
glmOutput = glm(my_catalog$topic==topic~.,data=data.frame(my_svd[,1:maxNumberOfDimensions]),family=binomial)
bestPredictors = rbind(bestPredictors,data.frame(t(sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]),ind=T,decreasing=T)$ix[1:maxNumberOfDimensions]))) #TODO: fix this. 2 is here to skip intercept.
predictorRatings <- rbind(predictorRatings, glmOutput$coefficients[2:length(glmOutput$coefficients)][sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]), ind = T, decreasing = T)$ix])
}
row.names(bestPredictors) <- topicList
row.names(predictorRatings) <- topicList
##STRATIFIED SAMPLING##
#for each topic, fill the "best predictors" matrix with the dimensions that most differentiate that theory from the other theories in the cluster. Cluster 1 is "Classic" with computational, bayesian and connectionist. Cluster 2 is "alt" with ecological, embodied, dynamical, distributed, enactive.
topicListClassic <- c(topicList[1], topicList[2], topicList[8]) # topic list of classical cluster.
topicListAlt <- setdiff(topicList, topicListClassic) #topic list of alt cluster.
catalogClassic <- my_catalog[which(my_catalog$topic %in% topicListClassic),] #catalog of classic papers
catalogAlt <- my_catalog[which(my_catalog$topic %in% topicListAlt),] #catalog of alt papers
#then, same procedure as above but using these clusters#
bestPredictorsClusters <- c()
predictorRatingsClusters <- c()
for (topic in topicListClassic) {
glmOutput = glm(catalogClassic$topic==topic~.,data=data.frame(my_svd[which(my_catalog$topic %in% topicListClassic, arr.ind = T),1:maxNumberOfDimensions]),family=binomial)
bestPredictorsClusters = rbind(bestPredictorsClusters,data.frame(t(sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]),ind=T,decreasing=T)$ix[1:maxNumberOfDimensions]))) #TODO: fix this. 2 is here to skip intercept.
predictorRatingsClusters <- rbind(predictorRatingsClusters, glmOutput$coefficients[2:length(glmOutput$coefficients)][sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]), ind = T, decreasing = T)$ix])
}
for (topic in topicListAlt) {
glmOutput = glm(catalogAlt$topic==topic~.,data=data.frame(my_svd[which(my_catalog$topic %in% topicListAlt, arr.ind = T),1:maxNumberOfDimensions]),family=binomial)
bestPredictorsClusters = rbind(bestPredictorsClusters,data.frame(t(sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]),ind=T,decreasing=T)$ix[1:maxNumberOfDimensions]))) #TODO: fix this. 2 is here to skip intercept.
predictorRatingsClusters <- rbind(predictorRatingsClusters, glmOutput$coefficients[2:length(glmOutput$coefficients)][sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]), ind = T, decreasing = T)$ix])
}
row.names(bestPredictorsClusters) <- c(topicListClassic, topicListAlt)
row.names(predictorRatingsClusters) <- c(topicListClassic, topicListAlt)
### EXECUTION OF MODEL ###
# Each theory has a model built and trained using the dimensions specified in minNumberOfDimensions and maxNumberOfDimensions. Each model is of the theories is a GLM. In each iteration, a random training set of 600 papers is selected i (random theory belonging if "free", 60 per theory if "cluster"). The remaining papers are presented to each GLM model and the probability returned by the model that the paper belongs to that theory is collected. The highest prediction value is selected as the "predicted" theory and stored. The results of this prediction are collected in each iteration. The number of iterations is specified in parameters above. The function is parallelized using the "foreach" and "doparallel" packages. "cl" controls the number of parallel processes; change to fit the number of cores in CPU. Each parallel iteration is one of the dimensions between minNumberOfDimensions and maxNumberOfDimensions #
dimensionVec <- c(minNumberOfDimensions:maxNumberOfDimensions)
# text file to monitor the parallel foreach #
logger = create.logger()
logfile(logger) = 'monitor.log'
level(logger) = 'INFO'
cl <- makeCluster(7)
registerDoParallel(cl)
listResults <- foreach(dimension=dimensionVec, .verbose = T, .packages = "log4r") %dopar% { #parallelized foreach loop with each dimension. Stored in a list object containing the aggregate matrices of iterations controlled in repeat, for each number of dimensions used.
resultsListModel <- lapply(c(1:repeats), matrix, nrow = 8, ncol = 8) #pre allocate the result list with the number of iterations selected in parameters.
s = 1 #controller for the number of iterations.
while(s <= repeats){ # repeats the process of training-prediction as specified in parameters.
if(method == "free") {
trainingSet = sample(1:nrow(my_svd),600) #not controlled training
predictors <- bestPredictors
}
if(method == "cluster"){
trainingSet <- c() # controlled training set for equal representation of each topic.
for(topic in topicList){
trainingSet <- c(trainingSet, sample(which(my_catalog$topic==topic), round(length(which(my_catalog$topic==topic)) * 0.7)))
}
predictors <- bestPredictorsClusters    }
if(source == "original" | source == "replication"){ #if the procedure is either predicting original for predicting original, or replication for predicting replication, set the trainingset as the rest of the papers.
testSet = setdiff(1:nrow(my_svd),trainingSet)
}
if(source == "cross"){ #if using original to predict replication, trainingset is original dataset, and testset is replication set.
trainingSet <- c(1:(nrow(my_svd)))
testSet <- c(1:(nrow(cross_svd)))
}
predictionResults = c()
for (topic in topicList) { #loop through the models of each theory
trainingdata <- data.frame(my_svd[trainingSet,unlist(predictors[topic,1:dimension])]) # prepare training data of model by using the dimensions selected as the best predictors for each topic and the documents selected to be training.
glmTopic = glm(my_catalog$topic[trainingSet]==topic~., data=trainingdata, family=binomial) #build the model of the topic.
if(source == "original" | source == "replication"){ #if predicting inside the dataset
testdata = data.frame(my_svd[testSet,unlist(predictors[topic,1:dimension])]) #prepare the data to be predicted
predicted = predict.glm(glmTopic,newdata=testdata,type="response") #store the probability that the paper belongs to the theory being tested
predictionResults = cbind(predictionResults,scale(predicted)) #add to a matrix and scale
}
if(source == "cross"){ #modify procedure above to account for original data being training and replication being test
predicted = predict.glm(glmTopic,newdata=data.frame(cross_svd[testSet,unlist(predictors[topic,1:dimension])]),type="response")
predictionResults = cbind(predictionResults,scale(predicted))
}
}
# the predictions of each theory are aggregated in a matrix. each row of matrix is a document, each column is the probability that it belongs to that theory.
predictionResults = data.frame(predictionResults)
colnames(predictionResults) = topicList
if(source == "original" | source == "replication"){
predictionResults$topic = my_catalog$topic[testSet] #add a column with the correct theory of each of the papers in the testset
predictionResults$predicted_topic = topicList[max.col(predictionResults[,1:8])] #add a column with the highest prediction of the models for that paper.
resultTable <- (table(predictionResults$topic,predictionResults$predicted_topic) / as.vector(table(my_catalog$topic[testSet])))*100 #generates a frequency table of how many times each topic was predicted as each other topic. then, transforms into percentages.
}
if(source == "cross") { #same procedure, but for cross prediction.
predictionResults$topic = cross_catalog$topic[testSet]
predictionResults$predicted_topic = topicList[max.col(predictionResults[,1:8])]
resultTable <- (table(predictionResults$topic,predictionResults$predicted_topic) / as.vector(table(cross_catalog$topic[testSet])))*100
}
resultsListModel[[s]] <- resultTable # store this iteration for final aggregation in list.
s = s + 1
if(s %% (round(repeats * 0.2
)) == 0){
info(logger, paste("dimension ", dimension, ", ", "iteration number", s))
}
}
#aggregate the results of the iterations#
finalPredictionTable <- matrix(0, nrow = 8, ncol = 8) #pre-allocate final table
#iterate through list of predictions to aggregate the results#
for(matrix in resultsListModel) { # sums every result table resulting from the iterations.
finalPredictionTable <- finalPredictionTable + matrix
}
finalPredictionTable <- finalPredictionTable / length(resultsListModel) #divide by total number of iterations to aggregate
return(finalPredictionTable) #return this table to be added to the list that foreach is constructing,
}
stopCluster(cl)
names(listResults) <- dimensionVec #name the objects in the list with the dimensions used in calculating them
dimEvMat <- matrix (0, nrow = length(dimensionVec), ncol = 9) # pre-allocate matrix for the evaluation of the effectiveness of models for each dimension
row.names(dimEvMat) <- as.character(dimensionVec) # name rows by dimension
colnames(dimEvMat) <- c(topicList, "mean") # name columns by each theory and designate a column to store the mean effectiveness
for(dimension in dimensionVec) { dimEvMat[as.character(dimension),] <- c(diag(listResults[[as.character(dimension)]]), mean(diag(listResults[[as.character(dimension)]])))} #fill list with the values of the diagonal of confusability matrices and its mean effectiveness.
###Plotting###
# scripts for producing the prediction related plots using ggplot2 #
## confusability matrix by dimension ##
dimension = 20 # parameter for choosing the number of dimensions to be used in the plot
topicMatrix <- listResults[[as.character(dimension)]] #extract the matrix of the chosen value of D
meltedResults <- melt(topicMatrix, varnames = c("Topic1", "Topic2"), value.name = "Percentage.Predicted")
heatmap <- ggplot(meltedResults, aes(y = Topic1, x = ordered(Topic2, levels = rev(sort(unique(Topic2)))))) + geom_tile(aes(fill = Percentage.Predicted)) + coord_equal() + scale_fill_gradient(limits = c(0, 100), low="white", high="seagreen", guide =  guide_colorbar(title = paste("% Predicted", "\n"))) + xlab("") + ylab("") + theme(axis.text = element_text(size = 14), axis.text.x = element_text(angle=330, hjust=0.4, vjust = 0.7, size = 14)) + geom_text(aes(label = paste(round(Percentage.Predicted, 1), "%", sep = "")), colour = "gray25", size = 5)
print(heatmap)
## horizontal heatmap of effectiveness of each dimension by topic and mean #
meltedDimEv <- melt(dimEvMat[, 1:9], varnames = c("D", "topic"), value.name = "Effectiveness") #melted effectiveness for ggplot2
heatmap <- ggplot(meltedDimEv, aes(y = topic, x = ordered(D))) + geom_tile(aes(fill = Effectiveness), color = "white") + coord_equal() + scale_fill_gradient(limits = c(0, 100), low="white", high="seagreen") + xlab("") + ylab("") + theme(axis.text = element_text(size = 12)) + geom_text(aes(label = paste(round(Effectiveness, 0))), size = 4, colour = "gray25")
print(heatmap)
documentLoadings <- wholeMacaroni$u %*% diag(wholeMacaroni$d)
termLoadings <- wholeMacaroni$v %*% diag(wholeMacaroni$d)
row.names(documentLoadings) <- row.names(cleanData)
row.names(termLoadings) <- colnames(cleanData)
termLoadings <- termLoadings[,1:30]
documentLoadings <- documentLoadings[, 1:30]
termVarimax <- varimax(termLoadings)
termLoadingsVarimax <- unclass(termVarimax$loadings)
documentLoadingsVarimax <- documentLoadings %*% termVarimax$rotmat
# Set parameters for the prediction. Min and max number of dimensions are used to control the number of dimensions that are to be used in the construction of the models. The procedure loops through the dimensions resulting from the SVD. Starts at minNumberOfDimensions (default: 3), stops at maxNumberOfDimensions (default: 50). "Method" refers to the data used to build and train the models. With "free", dimensions are selected for how well they predict theory belonging against every theory. With "cluster", the training is stratified to the "most similar" theories; e.g. "computational" is built using the dimensions that best predict computational papers when compared to 'bayesian' and 'connectionist'. "Repeats" is the number of iterations of the predicting process. "Source" controls which data set is to be used: "original" uses the original dataset, "replication" uses the replication data, and "cross" uses the projection of the replication data into the SVD space of the original dataset to predict their theories with the models built with the original dataset. #
minNumberOfDimensions <- 2 #lower boundary of D. does not work if lower than 2.
maxNumberOfDimensions <- 30 # upper boundary of D
repeats <- 100 # how many repetitions of prediction should be averaged?
method <- "free" # "cluster" or "free".
source <- "original" #"original" or "replication", "cross".
##OBJECTS##
# Loads the different objects depending on the parameter "source" on line 155.
if(source == "original"){
my_catalog <- catalog
my_svd <- documentLoadings
}
if(source == "replication"){
my_catalog <- repCatalog
my_svd <- repDocumentLoadings
}
if(source == "cross"){
my_catalog <- catalog
cross_catalog <- repCatalog
my_svd <- documentLoadings
replicationProjection <- matrix(0, nrow = nrow(repFreqMatrix), ncol = ncol(freqMatrix))   #create matrix for projection and populate#
row.names(replicationProjection) <- row.names(repFreqMatrix) #set row names as wholeMacaroni
colnames(replicationProjection) <- colnames(freqMatrix)
sharedWords <- colnames(repFreqMatrix)[which(colnames(repFreqMatrix) %in% colnames(freqMatrix))] # obtain all the shared words between original data and replication data.
replicationProjection[,sharedWords] <- repFreqMatrix[,sharedWords] #build a new DbT that has the documents in replication as rows and the shared words between both datasets as columns.
cross_svd <- replicationProjection %*% ginv(termLoadings) #projects the replication data into the SV  space of the original dataset..
}
### TOPIC BEST PREDICTORS ###
## At first, the dimensions that differentiate the papers of a theory when compared to the other ones are selected. The first 80 of them are stored. #
##FREE FOR ALL##
#for each topic, fill the "best predictors" matrix with the dimensions that most differentiate that theory from the other 8 theories.
bestPredictors <- c()
predictorRatings <- c() #store ratings to compare positive versus negative in term loading inspection
for (topic in topicList) {
glmOutput = glm(my_catalog$topic==topic~.,data=data.frame(my_svd[,1:maxNumberOfDimensions]),family=binomial)
bestPredictors = rbind(bestPredictors,data.frame(t(sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]),ind=T,decreasing=T)$ix[1:maxNumberOfDimensions]))) #TODO: fix this. 2 is here to skip intercept.
predictorRatings <- rbind(predictorRatings, glmOutput$coefficients[2:length(glmOutput$coefficients)][sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]), ind = T, decreasing = T)$ix])
}
row.names(bestPredictors) <- topicList
row.names(predictorRatings) <- topicList
##STRATIFIED SAMPLING##
#for each topic, fill the "best predictors" matrix with the dimensions that most differentiate that theory from the other theories in the cluster. Cluster 1 is "Classic" with computational, bayesian and connectionist. Cluster 2 is "alt" with ecological, embodied, dynamical, distributed, enactive.
topicListClassic <- c(topicList[1], topicList[2], topicList[8]) # topic list of classical cluster.
topicListAlt <- setdiff(topicList, topicListClassic) #topic list of alt cluster.
catalogClassic <- my_catalog[which(my_catalog$topic %in% topicListClassic),] #catalog of classic papers
catalogAlt <- my_catalog[which(my_catalog$topic %in% topicListAlt),] #catalog of alt papers
#then, same procedure as above but using these clusters#
bestPredictorsClusters <- c()
predictorRatingsClusters <- c()
for (topic in topicListClassic) {
glmOutput = glm(catalogClassic$topic==topic~.,data=data.frame(my_svd[which(my_catalog$topic %in% topicListClassic, arr.ind = T),1:maxNumberOfDimensions]),family=binomial)
bestPredictorsClusters = rbind(bestPredictorsClusters,data.frame(t(sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]),ind=T,decreasing=T)$ix[1:maxNumberOfDimensions]))) #TODO: fix this. 2 is here to skip intercept.
predictorRatingsClusters <- rbind(predictorRatingsClusters, glmOutput$coefficients[2:length(glmOutput$coefficients)][sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]), ind = T, decreasing = T)$ix])
}
for (topic in topicListAlt) {
glmOutput = glm(catalogAlt$topic==topic~.,data=data.frame(my_svd[which(my_catalog$topic %in% topicListAlt, arr.ind = T),1:maxNumberOfDimensions]),family=binomial)
bestPredictorsClusters = rbind(bestPredictorsClusters,data.frame(t(sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]),ind=T,decreasing=T)$ix[1:maxNumberOfDimensions]))) #TODO: fix this. 2 is here to skip intercept.
predictorRatingsClusters <- rbind(predictorRatingsClusters, glmOutput$coefficients[2:length(glmOutput$coefficients)][sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]), ind = T, decreasing = T)$ix])
}
row.names(bestPredictorsClusters) <- c(topicListClassic, topicListAlt)
row.names(predictorRatingsClusters) <- c(topicListClassic, topicListAlt)
### EXECUTION OF MODEL ###
# Each theory has a model built and trained using the dimensions specified in minNumberOfDimensions and maxNumberOfDimensions. Each model is of the theories is a GLM. In each iteration, a random training set of 600 papers is selected i (random theory belonging if "free", 60 per theory if "cluster"). The remaining papers are presented to each GLM model and the probability returned by the model that the paper belongs to that theory is collected. The highest prediction value is selected as the "predicted" theory and stored. The results of this prediction are collected in each iteration. The number of iterations is specified in parameters above. The function is parallelized using the "foreach" and "doparallel" packages. "cl" controls the number of parallel processes; change to fit the number of cores in CPU. Each parallel iteration is one of the dimensions between minNumberOfDimensions and maxNumberOfDimensions #
dimensionVec <- c(minNumberOfDimensions:maxNumberOfDimensions)
# text file to monitor the parallel foreach #
logger = create.logger()
logfile(logger) = 'monitor.log'
level(logger) = 'INFO'
cl <- makeCluster(7)
registerDoParallel(cl)
listResults <- foreach(dimension=dimensionVec, .verbose = T, .packages = "log4r") %dopar% { #parallelized foreach loop with each dimension. Stored in a list object containing the aggregate matrices of iterations controlled in repeat, for each number of dimensions used.
resultsListModel <- lapply(c(1:repeats), matrix, nrow = 8, ncol = 8) #pre allocate the result list with the number of iterations selected in parameters.
s = 1 #controller for the number of iterations.
while(s <= repeats){ # repeats the process of training-prediction as specified in parameters.
if(method == "free") {
trainingSet = sample(1:nrow(my_svd),600) #not controlled training
predictors <- bestPredictors
}
if(method == "cluster"){
trainingSet <- c() # controlled training set for equal representation of each topic.
for(topic in topicList){
trainingSet <- c(trainingSet, sample(which(my_catalog$topic==topic), round(length(which(my_catalog$topic==topic)) * 0.7)))
}
predictors <- bestPredictorsClusters    }
if(source == "original" | source == "replication"){ #if the procedure is either predicting original for predicting original, or replication for predicting replication, set the trainingset as the rest of the papers.
testSet = setdiff(1:nrow(my_svd),trainingSet)
}
if(source == "cross"){ #if using original to predict replication, trainingset is original dataset, and testset is replication set.
trainingSet <- c(1:(nrow(my_svd)))
testSet <- c(1:(nrow(cross_svd)))
}
predictionResults = c()
for (topic in topicList) { #loop through the models of each theory
trainingdata <- data.frame(my_svd[trainingSet,unlist(predictors[topic,1:dimension])]) # prepare training data of model by using the dimensions selected as the best predictors for each topic and the documents selected to be training.
glmTopic = glm(my_catalog$topic[trainingSet]==topic~., data=trainingdata, family=binomial) #build the model of the topic.
if(source == "original" | source == "replication"){ #if predicting inside the dataset
testdata = data.frame(my_svd[testSet,unlist(predictors[topic,1:dimension])]) #prepare the data to be predicted
predicted = predict.glm(glmTopic,newdata=testdata,type="response") #store the probability that the paper belongs to the theory being tested
predictionResults = cbind(predictionResults,scale(predicted)) #add to a matrix and scale
}
if(source == "cross"){ #modify procedure above to account for original data being training and replication being test
predicted = predict.glm(glmTopic,newdata=data.frame(cross_svd[testSet,unlist(predictors[topic,1:dimension])]),type="response")
predictionResults = cbind(predictionResults,scale(predicted))
}
}
# the predictions of each theory are aggregated in a matrix. each row of matrix is a document, each column is the probability that it belongs to that theory.
predictionResults = data.frame(predictionResults)
colnames(predictionResults) = topicList
if(source == "original" | source == "replication"){
predictionResults$topic = my_catalog$topic[testSet] #add a column with the correct theory of each of the papers in the testset
predictionResults$predicted_topic = topicList[max.col(predictionResults[,1:8])] #add a column with the highest prediction of the models for that paper.
resultTable <- (table(predictionResults$topic,predictionResults$predicted_topic) / as.vector(table(my_catalog$topic[testSet])))*100 #generates a frequency table of how many times each topic was predicted as each other topic. then, transforms into percentages.
}
if(source == "cross") { #same procedure, but for cross prediction.
predictionResults$topic = cross_catalog$topic[testSet]
predictionResults$predicted_topic = topicList[max.col(predictionResults[,1:8])]
resultTable <- (table(predictionResults$topic,predictionResults$predicted_topic) / as.vector(table(cross_catalog$topic[testSet])))*100
}
resultsListModel[[s]] <- resultTable # store this iteration for final aggregation in list.
s = s + 1
if(s %% (round(repeats * 0.2
)) == 0){
info(logger, paste("dimension ", dimension, ", ", "iteration number", s))
}
}
#aggregate the results of the iterations#
finalPredictionTable <- matrix(0, nrow = 8, ncol = 8) #pre-allocate final table
#iterate through list of predictions to aggregate the results#
for(matrix in resultsListModel) { # sums every result table resulting from the iterations.
finalPredictionTable <- finalPredictionTable + matrix
}
finalPredictionTable <- finalPredictionTable / length(resultsListModel) #divide by total number of iterations to aggregate
return(finalPredictionTable) #return this table to be added to the list that foreach is constructing,
}
stopCluster(cl)
names(listResults) <- dimensionVec #name the objects in the list with the dimensions used in calculating them
dimEvMat <- matrix (0, nrow = length(dimensionVec), ncol = 9) # pre-allocate matrix for the evaluation of the effectiveness of models for each dimension
row.names(dimEvMat) <- as.character(dimensionVec) # name rows by dimension
colnames(dimEvMat) <- c(topicList, "mean") # name columns by each theory and designate a column to store the mean effectiveness
for(dimension in dimensionVec) { dimEvMat[as.character(dimension),] <- c(diag(listResults[[as.character(dimension)]]), mean(diag(listResults[[as.character(dimension)]])))} #fill list with the values of the diagonal of confusability matrices and its mean effectiveness.
###Plotting###
# scripts for producing the prediction related plots using ggplot2 #
## confusability matrix by dimension ##
dimension = 20 # parameter for choosing the number of dimensions to be used in the plot
topicMatrix <- listResults[[as.character(dimension)]] #extract the matrix of the chosen value of D
meltedResults <- melt(topicMatrix, varnames = c("Topic1", "Topic2"), value.name = "Percentage.Predicted")
heatmap <- ggplot(meltedResults, aes(y = Topic1, x = ordered(Topic2, levels = rev(sort(unique(Topic2)))))) + geom_tile(aes(fill = Percentage.Predicted)) + coord_equal() + scale_fill_gradient(limits = c(0, 100), low="white", high="seagreen", guide =  guide_colorbar(title = paste("% Predicted", "\n"))) + xlab("") + ylab("") + theme(axis.text = element_text(size = 14), axis.text.x = element_text(angle=330, hjust=0.4, vjust = 0.7, size = 14)) + geom_text(aes(label = paste(round(Percentage.Predicted, 1), "%", sep = "")), colour = "gray25", size = 5)
print(heatmap)
## horizontal heatmap of effectiveness of each dimension by topic and mean #
meltedDimEv <- melt(dimEvMat[, 1:9], varnames = c("D", "topic"), value.name = "Effectiveness") #melted effectiveness for ggplot2
heatmap <- ggplot(meltedDimEv, aes(y = topic, x = ordered(D))) + geom_tile(aes(fill = Effectiveness), color = "white") + coord_equal() + scale_fill_gradient(limits = c(0, 100), low="white", high="seagreen") + xlab("") + ylab("") + theme(axis.text = element_text(size = 12)) + geom_text(aes(label = paste(round(Effectiveness, 0))), size = 4, colour = "gray25")
print(heatmap)
documentLoadings <- wholeMacaroni$u %*% diag(wholeMacaroni$d)
termLoadings <- wholeMacaroni$v %*% diag(wholeMacaroni$d)
row.names(documentLoadings) <- row.names(cleanData)
row.names(termLoadings) <- colnames(cleanData)
termLoadings <- termLoadings[,1:15]
documentLoadings <- documentLoadings[, 1:15]
minNumberOfDimensions <- 2 #lower boundary of D. does not work if lower than 2.
maxNumberOfDimensions <- 15 # upper boundary of D
repeats <- 100 # how many repetitions of prediction should be averaged?
method <- "free" # "cluster" or "free".
source <- "original" #"original" or "replication", "cross".
if(source == "original"){
my_catalog <- catalog
my_svd <- documentLoadings
}
if(source == "replication"){
my_catalog <- repCatalog
my_svd <- repDocumentLoadings
}
if(source == "cross"){
my_catalog <- catalog
cross_catalog <- repCatalog
my_svd <- documentLoadings
replicationProjection <- matrix(0, nrow = nrow(repFreqMatrix), ncol = ncol(freqMatrix))   #create matrix for projection and populate#
row.names(replicationProjection) <- row.names(repFreqMatrix) #set row names as wholeMacaroni
colnames(replicationProjection) <- colnames(freqMatrix)
sharedWords <- colnames(repFreqMatrix)[which(colnames(repFreqMatrix) %in% colnames(freqMatrix))] # obtain all the shared words between original data and replication data.
replicationProjection[,sharedWords] <- repFreqMatrix[,sharedWords] #build a new DbT that has the documents in replication as rows and the shared words between both datasets as columns.
cross_svd <- replicationProjection %*% ginv(termLoadings) #projects the replication data into the SV  space of the original dataset..
}
bestPredictors <- c()
predictorRatings <- c() #store ratings to compare positive versus negative in term loading inspection
termVarimax <- varimax(termLoadings)
termLoadingsVarimax <- unclass(termVarimax$loadings)
documentLoadingsVarimax <- documentLoadings %*% termVarimax$rotmat
minNumberOfDimensions <- 2 #lower boundary of D. does not work if lower than 2.
maxNumberOfDimensions <- 15 # upper boundary of D
repeats <- 100 # how many repetitions of prediction should be averaged?
method <- "free" # "cluster" or "free".
source <- "original" #"original" or "replication", "cross".
if(source == "original"){
my_catalog <- catalog
my_svd <- documentLoadings
}
if(source == "replication"){
my_catalog <- repCatalog
my_svd <- repDocumentLoadings
}
if(source == "cross"){
my_catalog <- catalog
cross_catalog <- repCatalog
my_svd <- documentLoadings
replicationProjection <- matrix(0, nrow = nrow(repFreqMatrix), ncol = ncol(freqMatrix))   #create matrix for projection and populate#
row.names(replicationProjection) <- row.names(repFreqMatrix) #set row names as wholeMacaroni
colnames(replicationProjection) <- colnames(freqMatrix)
sharedWords <- colnames(repFreqMatrix)[which(colnames(repFreqMatrix) %in% colnames(freqMatrix))] # obtain all the shared words between original data and replication data.
replicationProjection[,sharedWords] <- repFreqMatrix[,sharedWords] #build a new DbT that has the documents in replication as rows and the shared words between both datasets as columns.
cross_svd <- replicationProjection %*% ginv(termLoadings) #projects the replication data into the SV  space of the original dataset..
}
bestPredictors <- c()
predictorRatings <- c() #store ratings to compare positive versus negative in term loading inspection
for (topic in topicList) {
glmOutput = glm(my_catalog$topic==topic~.,data=data.frame(my_svd[,1:maxNumberOfDimensions]),family=binomial)
bestPredictors = rbind(bestPredictors,data.frame(t(sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]),ind=T,decreasing=T)$ix[1:maxNumberOfDimensions]))) #TODO: fix this. 2 is here to skip intercept.
predictorRatings <- rbind(predictorRatings, glmOutput$coefficients[2:length(glmOutput$coefficients)][sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]), ind = T, decreasing = T)$ix])
}
row.names(bestPredictors) <- topicList
row.names(predictorRatings) <- topicList
topicListClassic <- c(topicList[1], topicList[2], topicList[8]) # topic list of classical cluster.
topicListAlt <- setdiff(topicList, topicListClassic) #topic list of alt cluster.
catalogClassic <- my_catalog[which(my_catalog$topic %in% topicListClassic),] #catalog of classic papers
catalogAlt <- my_catalog[which(my_catalog$topic %in% topicListAlt),] #catalog of alt papers
bestPredictorsClusters <- c()
predictorRatingsClusters <- c()
for (topic in topicListClassic) {
glmOutput = glm(catalogClassic$topic==topic~.,data=data.frame(my_svd[which(my_catalog$topic %in% topicListClassic, arr.ind = T),1:maxNumberOfDimensions]),family=binomial)
bestPredictorsClusters = rbind(bestPredictorsClusters,data.frame(t(sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]),ind=T,decreasing=T)$ix[1:maxNumberOfDimensions]))) #TODO: fix this. 2 is here to skip intercept.
predictorRatingsClusters <- rbind(predictorRatingsClusters, glmOutput$coefficients[2:length(glmOutput$coefficients)][sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]), ind = T, decreasing = T)$ix])
}
for (topic in topicListAlt) {
glmOutput = glm(catalogAlt$topic==topic~.,data=data.frame(my_svd[which(my_catalog$topic %in% topicListAlt, arr.ind = T),1:maxNumberOfDimensions]),family=binomial)
bestPredictorsClusters = rbind(bestPredictorsClusters,data.frame(t(sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]),ind=T,decreasing=T)$ix[1:maxNumberOfDimensions]))) #TODO: fix this. 2 is here to skip intercept.
predictorRatingsClusters <- rbind(predictorRatingsClusters, glmOutput$coefficients[2:length(glmOutput$coefficients)][sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]), ind = T, decreasing = T)$ix])
}
row.names(bestPredictorsClusters) <- c(topicListClassic, topicListAlt)
row.names(predictorRatingsClusters) <- c(topicListClassic, topicListAlt)
dimensionVec <- c(minNumberOfDimensions:maxNumberOfDimensions)
logger = create.logger()
logfile(logger) = 'monitor.log'
level(logger) = 'INFO'
cl <- makeCluster(7)
registerDoParallel(cl)
listResults <- foreach(dimension=dimensionVec, .verbose = T, .packages = "log4r") %dopar% { #parallelized foreach loop with each dimension. Stored in a list object containing the aggregate matrices of iterations controlled in repeat, for each number of dimensions used.
resultsListModel <- lapply(c(1:repeats), matrix, nrow = 8, ncol = 8) #pre allocate the result list with the number of iterations selected in parameters.
s = 1 #controller for the number of iterations.
while(s <= repeats){ # repeats the process of training-prediction as specified in parameters.
if(method == "free") {
trainingSet = sample(1:nrow(my_svd),600) #not controlled training
predictors <- bestPredictors
}
if(method == "cluster"){
trainingSet <- c() # controlled training set for equal representation of each topic.
for(topic in topicList){
trainingSet <- c(trainingSet, sample(which(my_catalog$topic==topic), round(length(which(my_catalog$topic==topic)) * 0.7)))
}
predictors <- bestPredictorsClusters    }
if(source == "original" | source == "replication"){ #if the procedure is either predicting original for predicting original, or replication for predicting replication, set the trainingset as the rest of the papers.
testSet = setdiff(1:nrow(my_svd),trainingSet)
}
if(source == "cross"){ #if using original to predict replication, trainingset is original dataset, and testset is replication set.
trainingSet <- c(1:(nrow(my_svd)))
testSet <- c(1:(nrow(cross_svd)))
}
predictionResults = c()
for (topic in topicList) { #loop through the models of each theory
trainingdata <- data.frame(my_svd[trainingSet,unlist(predictors[topic,1:dimension])]) # prepare training data of model by using the dimensions selected as the best predictors for each topic and the documents selected to be training.
glmTopic = glm(my_catalog$topic[trainingSet]==topic~., data=trainingdata, family=binomial) #build the model of the topic.
if(source == "original" | source == "replication"){ #if predicting inside the dataset
testdata = data.frame(my_svd[testSet,unlist(predictors[topic,1:dimension])]) #prepare the data to be predicted
predicted = predict.glm(glmTopic,newdata=testdata,type="response") #store the probability that the paper belongs to the theory being tested
predictionResults = cbind(predictionResults,scale(predicted)) #add to a matrix and scale
}
if(source == "cross"){ #modify procedure above to account for original data being training and replication being test
predicted = predict.glm(glmTopic,newdata=data.frame(cross_svd[testSet,unlist(predictors[topic,1:dimension])]),type="response")
predictionResults = cbind(predictionResults,scale(predicted))
}
}
# the predictions of each theory are aggregated in a matrix. each row of matrix is a document, each column is the probability that it belongs to that theory.
predictionResults = data.frame(predictionResults)
colnames(predictionResults) = topicList
if(source == "original" | source == "replication"){
predictionResults$topic = my_catalog$topic[testSet] #add a column with the correct theory of each of the papers in the testset
predictionResults$predicted_topic = topicList[max.col(predictionResults[,1:8])] #add a column with the highest prediction of the models for that paper.
resultTable <- (table(predictionResults$topic,predictionResults$predicted_topic) / as.vector(table(my_catalog$topic[testSet])))*100 #generates a frequency table of how many times each topic was predicted as each other topic. then, transforms into percentages.
}
if(source == "cross") { #same procedure, but for cross prediction.
predictionResults$topic = cross_catalog$topic[testSet]
predictionResults$predicted_topic = topicList[max.col(predictionResults[,1:8])]
resultTable <- (table(predictionResults$topic,predictionResults$predicted_topic) / as.vector(table(cross_catalog$topic[testSet])))*100
}
resultsListModel[[s]] <- resultTable # store this iteration for final aggregation in list.
s = s + 1
if(s %% (round(repeats * 0.2
)) == 0){
info(logger, paste("dimension ", dimension, ", ", "iteration number", s))
}
}
#aggregate the results of the iterations#
finalPredictionTable <- matrix(0, nrow = 8, ncol = 8) #pre-allocate final table
#iterate through list of predictions to aggregate the results#
for(matrix in resultsListModel) { # sums every result table resulting from the iterations.
finalPredictionTable <- finalPredictionTable + matrix
}
finalPredictionTable <- finalPredictionTable / length(resultsListModel) #divide by total number of iterations to aggregate
return(finalPredictionTable) #return this table to be added to the list that foreach is constructing,
}
stopCluster(cl)
names(listResults) <- dimensionVec #name the objects in the list with the dimensions used in calculating them
dimEvMat <- matrix (0, nrow = length(dimensionVec), ncol = 9) # pre-allocate matrix for the evaluation of the effectiveness of models for each dimension
row.names(dimEvMat) <- as.character(dimensionVec) # name rows by dimension
colnames(dimEvMat) <- c(topicList, "mean") # name columns by each theory and designate a column to store the mean effectiveness
for(dimension in dimensionVec) { dimEvMat[as.character(dimension),] <- c(diag(listResults[[as.character(dimension)]]), mean(diag(listResults[[as.character(dimension)]])))} #fill list with the values of the diagonal of confusability matrices and its mean effectiveness.
dimension = 20 # parameter for choosing the number of dimensions to be used in the plot
topicMatrix <- listResults[[as.character(dimension)]] #extract the matrix of the chosen value of D
meltedResults <- melt(topicMatrix, varnames = c("Topic1", "Topic2"), value.name = "Percentage.Predicted")
heatmap <- ggplot(meltedResults, aes(y = Topic1, x = ordered(Topic2, levels = rev(sort(unique(Topic2)))))) + geom_tile(aes(fill = Percentage.Predicted)) + coord_equal() + scale_fill_gradient(limits = c(0, 100), low="white", high="seagreen", guide =  guide_colorbar(title = paste("% Predicted", "\n"))) + xlab("") + ylab("") + theme(axis.text = element_text(size = 14), axis.text.x = element_text(angle=330, hjust=0.4, vjust = 0.7, size = 14)) + geom_text(aes(label = paste(round(Percentage.Predicted, 1), "%", sep = "")), colour = "gray25", size = 5)
print(heatmap)
meltedDimEv <- melt(dimEvMat[, 1:9], varnames = c("D", "topic"), value.name = "Effectiveness") #melted effectiveness for ggplot2
heatmap <- ggplot(meltedDimEv, aes(y = topic, x = ordered(D))) + geom_tile(aes(fill = Effectiveness), color = "white") + coord_equal() + scale_fill_gradient(limits = c(0, 100), low="white", high="seagreen") + xlab("") + ylab("") + theme(axis.text = element_text(size = 12)) + geom_text(aes(label = paste(round(Effectiveness, 0))), size = 4, colour = "gray25")
print(heatmap)
bestPredictorsVarimax <- c()
predictorRatingsVarimax <- c() #store ratings to compare positive versus negative in term loading inspection
for (topic in topicList) {
glmOutput = glm(my_catalog$topic==topic~.,data=data.frame(documentLoadingsVarimax[,1:maxNumberOfDimensions]),family=binomial)
bestPredictorsVarimax = rbind(bestPredictorsVarimax,data.frame(t(sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]),ind=T,decreasing=T)$ix[1:maxNumberOfDimensions]))) #TODO: fix this. 2 is here to skip intercept.
predictorRatingsVarimax <- rbind(predictorRatingsVarimax, glmOutput$coefficients[2:length(glmOutput$coefficients)][sort(abs(glmOutput$coefficients[2:length(glmOutput$coefficients)]), ind = T, decreasing = T)$ix])
}
row.names(bestPredictorsVarimax) <- topicList
row.names(predictorRatingsVarimax) <- topicList
wordList <- list()
for(n in 1:length(topicList)){ # generate a data frame for each theory
topic <- topicList[n]
predictors <- unlist(bestPredictorsVarimax[topic,])
ratings <- unlist(predictorRatingsVarimax[topic,])
ratingsValence <- ratings > 0 # boolean for sign of the predictor. true if > 0, false if < 0
words <- c()
i = 1
for(i in c(1:length(predictors))){
dimension <- predictors[i]
words <- c(words, toString(head(names(sort(termLoadingsVarimax[,dimension], decreasing = T)), 150)))
}
nameOfDF <- paste(topic, "Meanings", sep = "")
assign(nameOfDF, data.frame(predictors, ratings, ratingsValence, words))
wordList[[n]] <- get(x = nameOfDF)
}
names(wordList) <- topicList
#wordclouds#
for(topic in topicList){
positiveWords <- c()
negativeWords <- c()
meanings <- wordList[[topic]]
for(i in 1:10){
if(meanings$ratingsValence[i]){
positiveWords <- c(positiveWords, unlist(strsplit(as.character(meanings$words[i]), ',')))
} else{
negativeWords <- c(negativeWords, unlist(strsplit(as.character(meanings$words[i]), ',')))
}
}
positivedf <- data.frame(names(sort(table(positiveWords), decreasing = T)), as.numeric(sort(table(positiveWords), decreasing = T)))
negativedf <- data.frame( names(sort(table(negativeWords), decreasing = T)), as.numeric(sort(table(negativeWords), decreasing = T)))
colnames(positivedf) <- c("word","freq")
colnames(negativedf) <- c("word", "freq")
png(filename = paste(topic, "PositiveCloud.png", sep = ""), units = "px", width = 3000, height = 2000)
wordcloud(positivedf$word, positivedf$freq, scale = c(7, .5), random.order = F, rot.per = 0, use.r.layout = F, min.freq = 2)
dev.off()
png(filename = paste(topic, "NegativeCloud.png", sep = ""), units = "px", width = 3000, height = 2000)
wordcloud(negativedf$word, negativedf$freq, scale = c(7, .5), random.order = F, rot.per = 0, use.r.layout = F, min.freq = 2)
dev.off()
}
write.csv(x = colnames(freqMatrix), "allWords.csv")
