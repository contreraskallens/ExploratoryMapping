#### libraries ####
setwd(dir = 'ExploratoryMapping/Scripts/')
require("MASS") #used to calculate the projection of new data in old SVD space.
# plotting #
require("reshape2")
require("ggplot2")
require("ggthemes")
require("scales")
# dendrogram tree cutting from https://labs.genetics.ucla.edu/horvath/CoexpressionNetwork/BranchCutting/ #
require("dynamicTreeCut")
require("bio3d")
require("moduleColor")
# paralellization of prediction machine and UI #
require("log4r")
require("foreach")
require("doParallel")
####FUNCTIONS####
cosineGen <- function(matrix){
lengthVec <- sqrt(rowSums(matrix * matrix))
tcrossprod(matrix) / (lengthVec %o% lengthVec)
}  #Function that generates the cosine between each row of a matrix.
calcEnt <- function(matrix){
workMatrix <- t(matrix) #transposes for division
a <- workMatrix / rowSums(workMatrix) #generates a probability matrix
b <- 1 + ((rowSums(a * log2(a), na.rm = T)) / log2(dim(matrix)[1])) #calculate entropy (1 + (sum of probability times log2 probability, divided by the total number of documents)).
workMatrix <- log(workMatrix[which(b > 0, arr.ind = T), ] + 1) #log normalizes frequency matrix and deletes 0 entropy ones.
workMatrix <- workMatrix * b[b > 0] #weight log normalized matrix by multiplying terms with entropy higher than 0 by its entropy.
return(t(workMatrix)) #returns original, non-transposed matrix.
} #calculates the entropy of each term of a matrix. Uses formula in Martin & Berry, 2007, "Mathematical foundations behind latent semantic analysis".
compareTheories <- function(matrix, cat){
##doc: takes matrix as the result of an svd(u). populates a pre-allocated list of every topic in external topicList (not generalized yet) with matrices of an "index" of similarity using each dimension (column) in matrix for each topic.
resultList <- lapply(lapply(1:8, matrix, data = 0, nrow = 8, ncol = dim(matrix)[2]), function(x){row.names(x) <- topicList; return(x)}) #pre-allocation of list
cosineAvgsList <- lapply(1:8, matrix, data = 0, nrow = 8, ncol = dim(matrix)[1]) #pre-allocation of second list
names(resultList) <- topicList #name for easy accessing theories
indexMatrix <- matrix(FALSE, nrow = dim(matrix)[1], ncol = 8, dimnames = list(1:dim(matrix)[1], topicList)) #pre-allocates logical matrix
for(topic in topicList){indexMatrix[,topic] <- cat[,2] == topic} #populates logical matrix with a logical mask reflecting catalog$id
docsByTop <- colSums(indexMatrix) #number of documents for each topic
n <- 1 #counter for each dimension
while(n <= dim(matrix)[2]){ #loops through dimensions
database <- matrix[, 1:n] #slices dimensions
if(n == 1){database <- cbind(database, 0)} #if it has only one dimension, then add a column of 0s to make cosineGen work
database <- cosineGen(database) #produces a x by x matrix of cosines between each paper.
database[is.na(database)] <- 0 #replaces NA with 0.
meanMatrix <- crossprod(indexMatrix, database) #produces a matrix with the sum of cosines of each paper with each of the topics
meanMatrix <- meanMatrix / docsByTop #produces a matrix with the mean cosine of each paper with each of the topics
cosineAvgsList[[n]] <- meanMatrix #stores the matrix of means in a list with n as index for dimensions used.
meanMatrix <- meanMatrix %*% indexMatrix #produces a vector with the sum of mean cosines for each topic against each topic in dimension n
meanMatrix <- t(meanMatrix) / docsByTop #produces a vector of the means of sums of mean cosines for each topic against each topic in dimension n.
for(topic in topicList){ #loops through topics to populate results of all cosines.
resultList[[topic]][, n] <- meanMatrix[topic,]
}
n = n + 1
}
returnList = list(resultList,cosineAvgsList) #makes list of lists with results and mean cosines
return(returnList) #returns everything
} #function for calculating cosines of the whole matrix. "matrix" is the result of an SVD; "cat" is the catalog to obtain topic information (in this case, catalog$id). Returns a list of two lists: [[1]] is all cosines by paper, [[2]] is a list of matrices of mean distance of each paper with each of the other topics. [[1]][n] and [[2]][n] are the different dimensions resulting from SVD.
plotTopicDiff <- function(topic, resultsList){
workTable <- as.data.frame(melt(resultsList[[1]][[topic]], varnames = c("topic", "dimension"), value.name = "cosine")) #long form for ggplot
plot <- ggplot(data = workTable, aes(x = dimension, y = cosine, color = topic, group = topic)) + theme_solarized(base_size = 14) + theme(axis.text = element_text(colour = "#586e75")) + labs(title = paste("Mean cosine of", topic, "papers with other theories and itself across dimensions")) + geom_line() + scale_colour_solarized("red") + geom_point(size = 0.7, shape = 3) + guides(colour = guide_legend(override.aes = list(size=3)))
print(plot)
} #function for plotting the mean distance of every topic with all other topics. "topic" is one of the topics of topicList; "resultsList" is the object that compareTheories() returns.
####DATA AND BASIC CLEANUP####
##ORIGINAL##
#loads files and catalog for original#
freqMatrix <- as.matrix(read.table('Data/Original/document_by_term.txt', sep='\t', header = T))[, -1] #loads the DBT minus one column, the identifier in the text file.
row.names(freqMatrix) <- c(1:nrow(freqMatrix)) #row names with docID
freqMatrix <- freqMatrix[, apply(freqMatrix, 2, function(x){sum(x==0) < (dim(freqMatrix)[1] - 5)})] #removes columns with words that appear in fewer than 5 documents.
cleanWords <- read.csv(file = 'allWords.csv', header = T, sep = ',')
freqMatrix <- freqMatrix[, cleanWords$Include.] #try with cleaner words
freqMatrix <- freqMatrix[which(rowSums(freqMatrix) > 0), ] #eliminates documents with 0 terms after cleanup of terminology
catalog <- read.table('catalog.txt', stringsAsFactors = F, sep = '\t', fill = T, quote = "") #loads catalog
catalog <- catalog[row.names(freqMatrix), ] #catalog also has row names as docID
colnames(catalog) = c('id','topic','year','authors','title','journal','abstract') #variable names for catalog
topicList <- unique(catalog$topic) #list of theories for analysis.
##REPLICATION##
#the same, but for replication documents#
repFreqMatrix <- as.matrix(read.table('rep_document_by_term.txt', sep='\t', header = T, quote = ""))[, -1]
row.names(repFreqMatrix) <- c(1:nrow(repFreqMatrix))
repFreqMatrix <- repFreqMatrix[, apply(repFreqMatrix, 2, function(x){sum(x==0) < (dim(repFreqMatrix)[1] - 5)})] #removes columns with words that appear in fewer than 5 documents.
repFreqMatrix <- repFreqMatrix[which(rowSums(repFreqMatrix) > 0), ]
repCatalog <- read.table('rep_catalog.txt', stringsAsFactors = F, sep = '\t', fill = T, quote = "")
repCatalog <- repCatalog[row.names(repFreqMatrix), ]
colnames(repCatalog) = c('id','topic','year','authors','title','journal','abstract')
topicList <- unique(repCatalog$topic)
#NULL HYPOTHESIS#
#If you want to test the null hypothesis, change the parameter to T. It randomizes the theory of the papers in the databases.#
nullHyp <- F
if(nullHyp == T ){
catalog$topic <- sample(catalog$topic, length(catalog$topic))
repCatalog$topic <- sample(repCatalog$topic, length(repCatalog$topic))
}
####DATA PROCESSING (Latent Semantic Analysis)####
##ENTROPY##
#this uses the entropy function in calcEnt to weight the matrices with a log-entropy function#
#ORIGINAL#
cleanData <- calcEnt(freqMatrix) #entropy
cleanData[is.na(cleanData)] <- 0 #replace NA with 0.
#REPLICATION#
repCleanData <- calcEnt(repFreqMatrix)
repCleanData[is.na(repCleanData)] <- 0
##SVD##
#ORIGINAL#
#dimensionality reduction#
wholeMacaroni = svd(cleanData) #SVD
#REPLICATION#
repofWholeMacaroni = svd(repCleanData)
#ALLOCATE LOADING MATRICES#
documentLoadings <- wholeMacaroni$u %*% diag(wholeMacaroni$d)
termLoadings <- wholeMacaroni$v %*% diag(wholeMacaroni$d)
row.names(documentLoadings) <- row.names(cleanData)
row.names(termLoadings) <- colnames(cleanData)
termLoadings <- termLoadings[,1:120]
documentLoadings <- documentLoadings[, 1:120]
#REPLICATION#
repDocumentLoadings <- repofWholeMacaroni$u %*% diag(repofWholeMacaroni$d)
repTermLoadings <- repofWholeMacaroni$v %*% diag(repofWholeMacaroni$d)
row.names(repDocumentLoadings) <- row.names(repCleanData)
row.names(repTermLoadings) <- colnames(repCleanData)
repTermLoadings <- repTermLoadings[,1:80]
repDocumentLoadings <- repDocumentLoadings[, 1:80]
setwd('GitHub/ExploratoryMapping/Scripts/')
#### libraries ####
setwd(dir = 'ExploratoryMapping/Scripts/')
require("MASS") #used to calculate the projection of new data in old SVD space.
# plotting #
require("reshape2")
require("ggplot2")
require("ggthemes")
require("scales")
# dendrogram tree cutting from https://labs.genetics.ucla.edu/horvath/CoexpressionNetwork/BranchCutting/ #
require("dynamicTreeCut")
require("bio3d")
require("moduleColor")
# paralellization of prediction machine and UI #
require("log4r")
require("foreach")
require("doParallel")
####FUNCTIONS####
cosineGen <- function(matrix){
lengthVec <- sqrt(rowSums(matrix * matrix))
tcrossprod(matrix) / (lengthVec %o% lengthVec)
}  #Function that generates the cosine between each row of a matrix.
calcEnt <- function(matrix){
workMatrix <- t(matrix) #transposes for division
a <- workMatrix / rowSums(workMatrix) #generates a probability matrix
b <- 1 + ((rowSums(a * log2(a), na.rm = T)) / log2(dim(matrix)[1])) #calculate entropy (1 + (sum of probability times log2 probability, divided by the total number of documents)).
workMatrix <- log(workMatrix[which(b > 0, arr.ind = T), ] + 1) #log normalizes frequency matrix and deletes 0 entropy ones.
workMatrix <- workMatrix * b[b > 0] #weight log normalized matrix by multiplying terms with entropy higher than 0 by its entropy.
return(t(workMatrix)) #returns original, non-transposed matrix.
} #calculates the entropy of each term of a matrix. Uses formula in Martin & Berry, 2007, "Mathematical foundations behind latent semantic analysis".
compareTheories <- function(matrix, cat){
##doc: takes matrix as the result of an svd(u). populates a pre-allocated list of every topic in external topicList (not generalized yet) with matrices of an "index" of similarity using each dimension (column) in matrix for each topic.
resultList <- lapply(lapply(1:8, matrix, data = 0, nrow = 8, ncol = dim(matrix)[2]), function(x){row.names(x) <- topicList; return(x)}) #pre-allocation of list
cosineAvgsList <- lapply(1:8, matrix, data = 0, nrow = 8, ncol = dim(matrix)[1]) #pre-allocation of second list
names(resultList) <- topicList #name for easy accessing theories
indexMatrix <- matrix(FALSE, nrow = dim(matrix)[1], ncol = 8, dimnames = list(1:dim(matrix)[1], topicList)) #pre-allocates logical matrix
for(topic in topicList){indexMatrix[,topic] <- cat[,2] == topic} #populates logical matrix with a logical mask reflecting catalog$id
docsByTop <- colSums(indexMatrix) #number of documents for each topic
n <- 1 #counter for each dimension
while(n <= dim(matrix)[2]){ #loops through dimensions
database <- matrix[, 1:n] #slices dimensions
if(n == 1){database <- cbind(database, 0)} #if it has only one dimension, then add a column of 0s to make cosineGen work
database <- cosineGen(database) #produces a x by x matrix of cosines between each paper.
database[is.na(database)] <- 0 #replaces NA with 0.
meanMatrix <- crossprod(indexMatrix, database) #produces a matrix with the sum of cosines of each paper with each of the topics
meanMatrix <- meanMatrix / docsByTop #produces a matrix with the mean cosine of each paper with each of the topics
cosineAvgsList[[n]] <- meanMatrix #stores the matrix of means in a list with n as index for dimensions used.
meanMatrix <- meanMatrix %*% indexMatrix #produces a vector with the sum of mean cosines for each topic against each topic in dimension n
meanMatrix <- t(meanMatrix) / docsByTop #produces a vector of the means of sums of mean cosines for each topic against each topic in dimension n.
for(topic in topicList){ #loops through topics to populate results of all cosines.
resultList[[topic]][, n] <- meanMatrix[topic,]
}
n = n + 1
}
returnList = list(resultList,cosineAvgsList) #makes list of lists with results and mean cosines
return(returnList) #returns everything
} #function for calculating cosines of the whole matrix. "matrix" is the result of an SVD; "cat" is the catalog to obtain topic information (in this case, catalog$id). Returns a list of two lists: [[1]] is all cosines by paper, [[2]] is a list of matrices of mean distance of each paper with each of the other topics. [[1]][n] and [[2]][n] are the different dimensions resulting from SVD.
plotTopicDiff <- function(topic, resultsList){
workTable <- as.data.frame(melt(resultsList[[1]][[topic]], varnames = c("topic", "dimension"), value.name = "cosine")) #long form for ggplot
plot <- ggplot(data = workTable, aes(x = dimension, y = cosine, color = topic, group = topic)) + theme_solarized(base_size = 14) + theme(axis.text = element_text(colour = "#586e75")) + labs(title = paste("Mean cosine of", topic, "papers with other theories and itself across dimensions")) + geom_line() + scale_colour_solarized("red") + geom_point(size = 0.7, shape = 3) + guides(colour = guide_legend(override.aes = list(size=3)))
print(plot)
} #function for plotting the mean distance of every topic with all other topics. "topic" is one of the topics of topicList; "resultsList" is the object that compareTheories() returns.
####DATA AND BASIC CLEANUP####
##ORIGINAL##
#loads files and catalog for original#
freqMatrix <- as.matrix(read.table('Data/Original/document_by_term.txt', sep='\t', header = T))[, -1] #loads the DBT minus one column, the identifier in the text file.
row.names(freqMatrix) <- c(1:nrow(freqMatrix)) #row names with docID
freqMatrix <- freqMatrix[, apply(freqMatrix, 2, function(x){sum(x==0) < (dim(freqMatrix)[1] - 5)})] #removes columns with words that appear in fewer than 5 documents.
cleanWords <- read.csv(file = 'allWords.csv', header = T, sep = ',')
freqMatrix <- freqMatrix[, cleanWords$Include.] #try with cleaner words
freqMatrix <- freqMatrix[which(rowSums(freqMatrix) > 0), ] #eliminates documents with 0 terms after cleanup of terminology
catalog <- read.table('catalog.txt', stringsAsFactors = F, sep = '\t', fill = T, quote = "") #loads catalog
catalog <- catalog[row.names(freqMatrix), ] #catalog also has row names as docID
colnames(catalog) = c('id','topic','year','authors','title','journal','abstract') #variable names for catalog
topicList <- unique(catalog$topic) #list of theories for analysis.
##REPLICATION##
#the same, but for replication documents#
repFreqMatrix <- as.matrix(read.table('rep_document_by_term.txt', sep='\t', header = T, quote = ""))[, -1]
row.names(repFreqMatrix) <- c(1:nrow(repFreqMatrix))
repFreqMatrix <- repFreqMatrix[, apply(repFreqMatrix, 2, function(x){sum(x==0) < (dim(repFreqMatrix)[1] - 5)})] #removes columns with words that appear in fewer than 5 documents.
repFreqMatrix <- repFreqMatrix[which(rowSums(repFreqMatrix) > 0), ]
repCatalog <- read.table('rep_catalog.txt', stringsAsFactors = F, sep = '\t', fill = T, quote = "")
repCatalog <- repCatalog[row.names(repFreqMatrix), ]
colnames(repCatalog) = c('id','topic','year','authors','title','journal','abstract')
topicList <- unique(repCatalog$topic)
#NULL HYPOTHESIS#
#If you want to test the null hypothesis, change the parameter to T. It randomizes the theory of the papers in the databases.#
nullHyp <- F
if(nullHyp == T ){
catalog$topic <- sample(catalog$topic, length(catalog$topic))
repCatalog$topic <- sample(repCatalog$topic, length(repCatalog$topic))
}
####DATA PROCESSING (Latent Semantic Analysis)####
##ENTROPY##
#this uses the entropy function in calcEnt to weight the matrices with a log-entropy function#
#ORIGINAL#
cleanData <- calcEnt(freqMatrix) #entropy
cleanData[is.na(cleanData)] <- 0 #replace NA with 0.
#REPLICATION#
repCleanData <- calcEnt(repFreqMatrix)
repCleanData[is.na(repCleanData)] <- 0
##SVD##
#ORIGINAL#
#dimensionality reduction#
wholeMacaroni = svd(cleanData) #SVD
#REPLICATION#
repofWholeMacaroni = svd(repCleanData)
#ALLOCATE LOADING MATRICES#
documentLoadings <- wholeMacaroni$u %*% diag(wholeMacaroni$d)
termLoadings <- wholeMacaroni$v %*% diag(wholeMacaroni$d)
row.names(documentLoadings) <- row.names(cleanData)
row.names(termLoadings) <- colnames(cleanData)
termLoadings <- termLoadings[,1:120]
documentLoadings <- documentLoadings[, 1:120]
#REPLICATION#
repDocumentLoadings <- repofWholeMacaroni$u %*% diag(repofWholeMacaroni$d)
repTermLoadings <- repofWholeMacaroni$v %*% diag(repofWholeMacaroni$d)
row.names(repDocumentLoadings) <- row.names(repCleanData)
row.names(repTermLoadings) <- colnames(repCleanData)
repTermLoadings <- repTermLoadings[,1:80]
repDocumentLoadings <- repDocumentLoadings[, 1:80]
freqMatrix <- as.matrix(read.table('document_by_term.txt', sep='\t', header = T))[, -1] #loads the DBT minus one column, the identifier in the text file.
row.names(freqMatrix) <- c(1:nrow(freqMatrix)) #row names with docID
freqMatrix <- freqMatrix[, apply(freqMatrix, 2, function(x){sum(x==0) < (dim(freqMatrix)[1] - 5)})] #removes columns with words that appear in fewer than 5 documents.
cleanWords <- read.csv(file = 'allWords.csv', header = T, sep = ',')
freqMatrix <- freqMatrix[, cleanWords$Include.] #try with cleaner words
freqMatrix <- freqMatrix[which(rowSums(freqMatrix) > 0), ] #eliminates documents with 0 terms after cleanup of terminology
catalog <- read.table('catalog.txt', stringsAsFactors = F, sep = '\t', fill = T, quote = "") #loads catalog
catalog <- catalog[row.names(freqMatrix), ] #catalog also has row names as docID
colnames(catalog) = c('id','topic','year','authors','title','journal','abstract') #variable names for catalog
topicList <- unique(catalog$topic) #list of theories for analysis.
repFreqMatrix <- as.matrix(read.table('rep_document_by_term.txt', sep='\t', header = T, quote = ""))[, -1]
row.names(repFreqMatrix) <- c(1:nrow(repFreqMatrix))
repFreqMatrix <- repFreqMatrix[, apply(repFreqMatrix, 2, function(x){sum(x==0) < (dim(repFreqMatrix)[1] - 5)})] #removes columns with words that appear in fewer than 5 documents.
repFreqMatrix <- repFreqMatrix[which(rowSums(repFreqMatrix) > 0), ]
repCatalog <- read.table('rep_catalog.txt', stringsAsFactors = F, sep = '\t', fill = T, quote = "")
repCatalog <- repCatalog[row.names(repFreqMatrix), ]
colnames(repCatalog) = c('id','topic','year','authors','title','journal','abstract')
topicList <- unique(repCatalog$topic)
nullHyp <- F
if(nullHyp == T ){
catalog$topic <- sample(catalog$topic, length(catalog$topic))
repCatalog$topic <- sample(repCatalog$topic, length(repCatalog$topic))
}
cleanData <- calcEnt(freqMatrix) #entropy
cleanData[is.na(cleanData)] <- 0 #replace NA with 0.
repCleanData <- calcEnt(repFreqMatrix)
repCleanData[is.na(repCleanData)] <- 0
wholeMacaroni = svd(cleanData) #SVD
repofWholeMacaroni = svd(repCleanData)
documentLoadings <- wholeMacaroni$u %*% diag(wholeMacaroni$d)
termLoadings <- wholeMacaroni$v %*% diag(wholeMacaroni$d)
row.names(documentLoadings) <- row.names(cleanData)
row.names(termLoadings) <- colnames(cleanData)
termLoadings <- termLoadings[,1:120]
documentLoadings <- documentLoadings[, 1:120]
repDocumentLoadings <- repofWholeMacaroni$u %*% diag(repofWholeMacaroni$d)
repTermLoadings <- repofWholeMacaroni$v %*% diag(repofWholeMacaroni$d)
row.names(repDocumentLoadings) <- row.names(repCleanData)
row.names(repTermLoadings) <- colnames(repCleanData)
repTermLoadings <- repTermLoadings[,1:80]
repDocumentLoadings <- repDocumentLoadings[, 1:80]
minNumberOfDimensions <- 2 #lower boundary of D. does not work if lower than 2.
freqMatrixAllWords <- read.csv(file = 'document_by_term_all_words.txt')
View(freqMatrixAllWords)
freqMatrixAllWords <- read.csv(file = 'document_by_term_all_words.txt', header = T, sep = ',')
row.names(freqMatrixAllWords) <- c(1:nrow(freqMatrixAllWords)) #row names with docID
